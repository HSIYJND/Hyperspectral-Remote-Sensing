\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage{graphicx}
\begin{document}

\noindent
\large\textbf{Weekly Report} \hfill \textbf{Abhijeet Ghodgaonkar} \\
\normalsize 4379 Lab \hfill 
 \hfill Date: 11/07/18 \\


\section{Problem Statement}
To completely perform SVM and Random Forest on Open Datasets Indian Pines , Pavia University and Kennedy Space Center.

\section{Tasks}
\begin{itemize}

\item Implementing GridSearchCV() (and k-fold cross validation) to find the optimal hyperparameters in SVM and Random Forest Classifier.
\item Observe the nature of Discriminant functions in SVM
\item Perform k-fold cross validation selectively on the data
\item Dump/Pickle the optimal Model and indices of the class-wise segregated dataset into binary files , so that it need not be retrained again for a different dataset.

\end{itemize}

\section{Solutions}
Using SVM and Random Forest, the first task was to see the accuracy metrics by training the classifier functions by train test splitting the data. 
\newline
The main objectives here are to overcome the random splitting of data which is not suited for the hyperspectral datasets , and to see the ideal train test split ratio in order to minimize the variance in accuracy.
\newline
The next step to fine tune the training is to use k-fold cross validation and gridsearch to search for the optimal parameters.




\section{Code Implementation}
\includegraphics[width=15cm]{/home/abhi/Pictures/code.png}

\section{Analysis \& Further Work}

\subsection{Analysis}
\begin{itemize}
\item The accuracy with train-test split is less as compared to K-fold CV.
The optimal value of k is 10.

\item Doing GridSearchCV on SVM is easier to do than on Random Forest, due to the complicated parameters of the function 'sklearn.ensemble.RandomForestClassifier'.
\end{itemize}
\subsection{Further Work}
\begin{itemize}
\item To fine tune parameters for Random Forest Classifier
\item To use sklearn.pipeline to ensure that the model gets trained and its parameters gets updated in a pipeline , without having to feed inputs into outputs, and outputs into inputs.
\end{itemize}

\section*{Final Evaluation}
The classical methods are good only when the hyperparameters are fine-tuned, and the data is trained properly. 
\newline
This method will not be good for original remote sensing data as the boundaries or rules cannot be learned properly from the data, and neural network models have to be used to do classification.

\end{document}
